import json
from pathlib import Path
from typing import Dict, List, Optional, Tuple, Union

import pytest

from pymusas.lexicon_collection import LexiconCollection, LexiconType
from pymusas.rankers.lexical_match import LexicalMatch
from pymusas.rankers.ranking_meta_data import RankingMetaData
from pymusas.taggers.rules.single_word import SingleWordRule


DATA_DIR = Path(__file__, '..', '..', '..', 'data').resolve()
RULE_DATA_DIR = Path(DATA_DIR, 'taggers', 'rules', 'single_word')
NON_SPECIAL_LEXICON_FILE = Path(RULE_DATA_DIR, 'single_word_non_special_lexicon.tsv')
NON_SPECIAL_DATA_FILE = Path(RULE_DATA_DIR, 'single_word_non_special_input_output.json')
POS_MAPPED_NON_SPECIAL_DATA_FILE = Path(RULE_DATA_DIR, 'single_word_pos_mapped_non_special_input_output.json')


def generate_test_data(test_data_file: Path, lexicon_file: Path
                       ) -> Tuple[List[str],
                                  List[str],
                                  List[str],
                                  Dict[str, List[str]],
                                  Dict[str, List[str]],
                                  List[List[RankingMetaData]]
                                  ]:
    '''
    Given the test data stored at `test_data_file`, and the lexicon file
    at `lexicon_file`, it returns this data as a Tuple of length 6:

    1. A List of `tokens`, from the `test_data_file`.
    2. A List of `lemmas`, from the `test_data_file`.
    3. A List of `POS tags`, from the `test_data_file`.
    4. The lexicon generated by parsing the `lexicon_file` to the
    `pymusas.lexicon_collection.LexiconCollection.from_tsv` method.
    5. The same as 4, but excluding Part Of Speech (POS) information.
    6. A list of a list of expected
    :class:`pymusas.rankers.lexicon_entry.RankingMetaData` objects.

    # Parameters

    test_data_file : `Path`
        A JSON file containing an Array of Objects. Each object must contain the
        following properties/keys:
        1. token, type str
        2. lemma, type str
        3. pos, type str
        4. ranking_meta_data_objects, type List[List[RankingMetaData]] - This
        has to be written as a JSON object that is then converted to a
        RankingMetaData object in Python.

    lexicon_file : `Path`
        A TSV file that can be converted into a
        :class:`pymusas.lexicon_collection.LexiconCollection` by using the
        class method :func:`pymusas.lexicon_collection.LexiconCollection.from_tsv`
    
    # Returns

    `Tuple[List[str], List[str], List[str], Dict[str, List[str]],
           Dict[str, List[str]], List[List[RankingMetaData]]]`
    '''
    def json_to_ranking_meta_data(json_object: Dict[str, Union[str, int,
                                                               bool, List[str]]]
                                  ) -> RankingMetaData:
        
        assert isinstance(json_object['lexicon_type'], str)
        lexicon_type = getattr(LexiconType, json_object['lexicon_type'])
        assert isinstance(lexicon_type, LexiconType)

        n_gram_length = json_object['lexicon_n_gram_length']
        assert isinstance(n_gram_length, int)

        wildcard_count = json_object['wildcard_count']
        assert isinstance(wildcard_count, int)

        exclude_pos_information = json_object['exclude_pos_information']
        assert isinstance(exclude_pos_information, bool)

        assert isinstance(json_object['lexical_match'], str)
        lexical_match = getattr(LexicalMatch, json_object['lexical_match'])
        assert isinstance(lexical_match, LexicalMatch)

        start_index = json_object['token_match_start_index']
        assert isinstance(start_index, int)

        end_index = json_object['token_match_end_index']
        assert isinstance(end_index, int)

        lexicon_entry_match = json_object['lexicon_entry_match']
        assert isinstance(lexicon_entry_match, str)

        semantic_tags_list = json_object['semantic_tags']
        assert isinstance(semantic_tags_list, list)
        for value in semantic_tags_list:
            assert isinstance(value, str)
        semantic_tags = tuple(semantic_tags_list)

        return RankingMetaData(lexicon_type, n_gram_length, wildcard_count,
                               exclude_pos_information, lexical_match,
                               start_index, end_index, lexicon_entry_match,
                               semantic_tags)
    
    test_tokens: List[str] = []
    test_lemmas: List[str] = []
    test_pos_tags: List[str] = []
    test_ranking_meta_data: List[List[RankingMetaData]] = []
    
    with test_data_file.open('r') as test_data_fp:
        for token_data in json.load(test_data_fp):
            test_tokens.append(token_data['token'])
            test_lemmas.append(token_data['lemma'])
            test_pos_tags.append(token_data['pos'])
            
            token_ranking_meta_data: List[RankingMetaData] = []
            ranking_meta_data_objects = token_data['ranking_meta_data_objects']
            for ranking_object in ranking_meta_data_objects:
                ranking_object = json_to_ranking_meta_data(ranking_object)
                token_ranking_meta_data.append(ranking_object)
            test_ranking_meta_data.append(token_ranking_meta_data)
            
    lexicon_lookup = LexiconCollection.from_tsv(lexicon_file)
    lemma_lexicon_lookup = LexiconCollection.from_tsv(lexicon_file,
                                                      include_pos=False)
    
    return (test_tokens, test_lemmas, test_pos_tags, lexicon_lookup,
            lemma_lexicon_lookup, test_ranking_meta_data)


def compare_token_ranking_meta_data(token_ranking_meta_data_1: List[List[RankingMetaData]],
                                    token_ranking_meta_data_2: List[List[RankingMetaData]]
                                    ) -> None:
    '''
    This tests if the two token ranking meta data lists are equal to each other.

    # Raises

    `AssertionError`
        If the two lists are not of same length.
    `AssertionError`
        If each inner list is not of same length.
    `AssertionError`
        If each inner list when converted to a set are not equal to each other.
    '''
    assert len(token_ranking_meta_data_1) == len(token_ranking_meta_data_2)

    index = 0
    for ranking_meta_data_1, ranking_meta_data_2 in zip(token_ranking_meta_data_1,
                                                        token_ranking_meta_data_2):
        assert len(ranking_meta_data_1) == len(ranking_meta_data_2), index
        assert set(ranking_meta_data_1) == set(ranking_meta_data_2), index


@pytest.mark.parametrize('from_bytes', [False, True])
def test_single_word_rule__NON_SPECIAL_CASES(from_bytes: bool) -> None:
    '''
    This tests Single Word Rule when using only NON SPECIAL CASES,
    which are direct matches, i.e. does not use any special syntax
    like wildcards.
    '''
    # Test that it returns a list of one empty list, as we have no tokens to
    # tag
    empty_rule = SingleWordRule({}, {})
    assert [] == empty_rule([], [], [])

    (tokens, lemmas, pos_tags, lexicon, lemma_lexicon,
     expected_ranking_meta_data) = generate_test_data(NON_SPECIAL_DATA_FILE,
                                                      NON_SPECIAL_LEXICON_FILE)
    
    single_rule = SingleWordRule(lexicon, lemma_lexicon)
    
    if from_bytes:
        single_rule = SingleWordRule.from_bytes(single_rule.to_bytes())
    
    compare_token_ranking_meta_data(expected_ranking_meta_data,
                                    single_rule(tokens, lemmas, pos_tags))


@pytest.mark.parametrize('from_bytes', [False, True])
def test_single_word_rule_pos_mapper__NON_SPECIAL_CASES(from_bytes: bool
                                                        ) -> None:
    '''
    This tests Single Word Rule using the `pos_mapper` when using only
    NON SPECIAL CASES, which are direct matches, i.e. does not use any
    special syntax like wildcards.
    '''
    (tokens, lemmas, pos_tags, lexicon, lemma_lexicon,
     expected_ranking_meta_data) = generate_test_data(POS_MAPPED_NON_SPECIAL_DATA_FILE,
                                                      NON_SPECIAL_LEXICON_FILE)
    pos_mapper = {'NN': ['adv', 'noun']}
    single_rule = SingleWordRule(lexicon, lemma_lexicon, pos_mapper)
    
    if from_bytes:
        single_rule = SingleWordRule.from_bytes(single_rule.to_bytes())
    
    compare_token_ranking_meta_data(expected_ranking_meta_data,
                                    single_rule(tokens, lemmas, pos_tags))


@pytest.mark.parametrize("pos_mapper", [None, {'NN': ['adv', 'noun']}])
def test_to_from_bytes(pos_mapper: Optional[Dict[str, List[str]]]) -> None:

    (_, _, _, lexicon, lemma_lexicon, _) = generate_test_data(NON_SPECIAL_DATA_FILE,
                                                              NON_SPECIAL_LEXICON_FILE)
    single_rule = SingleWordRule(lexicon, lemma_lexicon, pos_mapper)
    single_rule_from_bytes = SingleWordRule.from_bytes(single_rule.to_bytes())

    assert single_rule.pos_mapper == single_rule_from_bytes.pos_mapper
    assert single_rule.lexicon_collection.data \
        == single_rule_from_bytes.lexicon_collection.data
    assert single_rule.lemma_lexicon_collection.data \
        == single_rule_from_bytes.lemma_lexicon_collection.data


def test__eq__() -> None:
    (_, _, _, lexicon, lemma_lexicon, _) = generate_test_data(NON_SPECIAL_DATA_FILE,
                                                              NON_SPECIAL_LEXICON_FILE)
    pos_mapper = {'NN': ['adv', 'noun']}

    empty_rule = SingleWordRule(lexicon, lemma_lexicon, pos_mapper)
    assert empty_rule == SingleWordRule(lexicon, lemma_lexicon, pos_mapper)

    empty_rule = SingleWordRule({}, {}, None)
    assert empty_rule != SingleWordRule(lexicon, {}, None)
    assert empty_rule != SingleWordRule({}, lemma_lexicon, None)
    assert empty_rule != SingleWordRule({}, {}, pos_mapper)
    assert 1 != empty_rule
