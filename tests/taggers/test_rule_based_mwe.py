import json
from pathlib import Path
from typing import List, Dict, Tuple

import pytest

from pymusas.lexicon_collection import MWELexiconCollection
from pymusas.taggers.rule_based_mwe import n_grams, n_gram_indexes, _tag_mwe


DATA_DIR = Path(__file__, '..', '..', 'data').resolve()
TAGGER_DATA_DIR = Path(DATA_DIR, 'taggers')

BASIC_LEXICON = Path(TAGGER_DATA_DIR, 'mwe_basic_lexicon.tsv')
BASIC_DATA = Path(TAGGER_DATA_DIR, 'rule_based_mwe_basic_input_output.json')


def generate_tag_test_data(test_data_file: Path, mwe_lexicon_file: Path
                           ) -> Tuple[List[str],
                                      List[str],
                                      List[str],
                                      Dict[str, List[str]],
                                      List[List[str]],
                                      List[int]]:
    '''
    Given the test data stored at `test_data_file`, and
    the MWE lexicon at `mwe_lexicon_file`, it returns this data as a
    Tuple of length 6:

    1. A List of `tokens`, from the `test_data_file`.
    2. A List of `lemmas`, from the `test_data_file`.
    3. A List of `POS tags`, from the `test_data_file`.
    4. The MWE lexicon generated by parsing the `mwe_lexicon_file` to the
    `pymusas.lexicon_collection.MWELexiconCollection.from_tsv` method.
    5. A list of a list of expected semantic tags that should be generated based
    on the associated `token`, `lemma`, and `pos` from the first value of the tuple and
    the MWE lexicon data from the second tuple value.
    6. A list of expected `mwe_id` whereby each id defines which MWE a token belongs
    too, an id of 0 represents a token that is not part of an MWE.

    # Parameters

    test_data_file : `Path`
        A JSON file containing an Array of Objects. Each object must contain the
        following properties/keys:
        1. token, type str
        2. lemma, type str
        3. pos, type str
        4. usas, type str
        5. mwe_id, type int

    mwe_lexicon_file : `Path`
        A TSV file that can be converted into a :class:`pymusas.lexicon_collection.MWELexiconCollection`
        by using the class method :func:`pymusas.lexicon_collection.MWELexiconCollection.from_tsv`
    
    # Returns

    `Tuple[List[str], List[str], List[str], Dict[str, List[str]], List[List[str]], List[int]]`
    '''
    test_tokens: List[str] = []
    test_lemmas: List[str] = []
    test_pos_tags: List[str] = []
    
    expected_usas_tags: List[List[str]] = []
    expected_mwe_ids: List[int] = []
    with test_data_file.open('r') as test_data_fp:
        for token_data in json.load(test_data_fp):
            test_tokens.append(token_data['token'])
            test_lemmas.append(token_data['lemma'])
            test_pos_tags.append(token_data['pos'])
            expected_usas_tags.append([token_data['usas']])
            expected_mwe_ids.append(token_data['mwe_id'])
    
    lexicon_lookup = MWELexiconCollection.from_tsv(mwe_lexicon_file)
    
    return (test_tokens, test_lemmas, test_pos_tags, lexicon_lookup,
            expected_usas_tags, expected_mwe_ids)


def test_tag_mwe_tokens__basic_rules() -> None:
    '''
    This tests the tag_mwe_tokens function with the basic rules that comes from
    using the `mwe_basic_lexicons` and `rules_based_mwe_basic_input_output` files.

    The basic rules for the MWE templates are:
    
    Starting with the longest n-gram templates assign semantic tags to tokens
    without semantic tags in the following order:
        1. Match on tokens and POS tags.
        2. Match on lemma and POS tags.
        3. Match on lower cased tokens and POS tags.
        4. Match on lower cased lemmas and POS tags.
    Then repeat this process for `n = n-1`. Stop when `n==2`, e.g. a
    MWE has to have at last 2 tokens.
    '''
    (tokens, lemmas, pos_tags, mwe_lexicon,
     expected_usas, expected_mwe_ids) = generate_tag_test_data(BASIC_DATA, BASIC_LEXICON)
    
    # Test that it returns all Z99 when we have no MWE rules
    empty_mwe_lexicon: Dict[str, List[str]] = {}
    usas_tags, mwe_ids = _tag_mwe(tokens, lemmas, pos_tags, empty_mwe_lexicon, 3)
    all_z99_tags = [['Z99'] for _ in tokens]
    all_0_ids = [0 for _ in tokens]
    assert all_z99_tags == usas_tags
    assert all_0_ids == mwe_ids

    # Test that it covers all of the non special syntax cases, e.g. all of the
    # cases that do not contain a wildcard or curly braces.
    usas_tags, mwe_ids = _tag_mwe(tokens, lemmas, pos_tags, mwe_lexicon, 3)
    assert expected_usas == usas_tags
    assert expected_mwe_ids == mwe_ids


def test_n_grams() -> None:
    test_tokens: List[str] = []
    empty_n_grams = n_grams(test_tokens, 1, 3)
    assert [] == list(empty_n_grams)

    test_tokens.extend(['hello', 'how', 'are', 'you', ','])
    expected_n_grams = [['hello'], ['how'], ['are'], ['you'], [',']]
    assert expected_n_grams == list(n_grams(test_tokens, 1, 1))

    expected_n_grams = [['hello', 'how', 'are'], ['how', 'are', 'you'], ['are', 'you', ','],
                        ['hello', 'how'], ['how', 'are'], ['are', 'you'], ['you', ',']]
    assert expected_n_grams == list(n_grams(test_tokens, 2, 3))

    assert [['hello', 'how', 'are', 'you', ',']] == list(n_grams(test_tokens, 5, 8))
    
    assert [] == list(n_grams(test_tokens, 6, 8))

    with pytest.raises(ValueError):
        list(n_grams(test_tokens, 0, 1))

    with pytest.raises(ValueError):
        list(n_grams(test_tokens, 2, 1))


def test_n_gram_indexes() -> None:
    test_tokens: List[str] = []
    empty_n_grams = n_gram_indexes(test_tokens, 1, 3)
    assert [] == list(empty_n_grams)

    test_tokens.extend(['hello', 'how', 'are', 'you', ','])
    expected_n_gram_indexes = [(0, 1), (1, 2), (2, 3), (3, 4), (4, 5)]
    assert expected_n_gram_indexes == list(n_gram_indexes(test_tokens, 1, 1))

    expected_n_gram_indexes = [(0, 3), (1, 4), (2, 5), (0, 2), (1, 3), (2, 4), (3, 5)]
    assert expected_n_gram_indexes == list(n_gram_indexes(test_tokens, 2, 3))

    assert [(0, 5)] == list(n_gram_indexes(test_tokens, 5, 8))
    
    assert [] == list(n_gram_indexes(test_tokens, 6, 8))

    with pytest.raises(ValueError):
        list(n_gram_indexes(test_tokens, 0, 1))

    with pytest.raises(ValueError):
        list(n_gram_indexes(test_tokens, 2, 1))
